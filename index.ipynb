{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for prof:\n",
    "\n",
    "1. Business case\n",
    "2. Structure: created pipeline w/ flags and variables, then created function to call pipeline builder and evaluate\n",
    "3. What type of model to use? How many steps? Try multiple models, # and type of steps? \n",
    "4. How to evaluate? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine neutral and negative to make a binary scenario\n",
    "\n",
    "apple to build a product, wants to highlight positive tweets for marketing\n",
    "or highlighting certain ones for later analysis to see what people like about this\n",
    "\n",
    "\n",
    "choose a model type, baseline, \n",
    "\n",
    "80, 16, 4 - train, val, test \n",
    "\n",
    "decision tree, canon, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "A new stealth tech company is building a new product. They have hired me to gather the positive comments made on Twitter about Google and Apple products to help them understand people's favorite aspects of those products, so they can incorporate them into their new product. The tool will flag future tweets as either positive or non-positive (could be neutral or negative) for deeper analysis. The company has stated that they would like to flag as many positive tweets as possible while minimizing false positives (tweets incorrectly labeled as positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['tweet_text'], inplace=True)\n",
    "df.drop_duplicates(subset=['tweet_text'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and neutral flags and remove \"I can't tell\"\n",
    "\n",
    "neg_neut_combine = df[df['is_there_an_emotion_directed_at_a_brand_or_product'] != \"I can't tell\"]\n",
    "neg_neut_combine['pos_flag'] = 0\n",
    "neg_neut_combine.loc[df['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion', 'pos_flag'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_neut_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_emo_counts = neg_neut_combine['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2968/(5372+2968+569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['No emotion', 'Positive', 'Negative']\n",
    "total_emo_counts = neg_neut_combine['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()\n",
    "plt.bar(categories, neg_neut_emo_counts)\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Count')\n",
    "plt.title('Emotion Counts')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_neut_combine[neg_neut_combine['emotion_in_tweet_is_directed_at'].isnull()]['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['No emotion / Negative', 'Positive']\n",
    "neg_neut_emo_counts = neg_neut_combine['pos_flag'].value_counts()\n",
    "plt.bar(categories, neg_neut_emo_counts)\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Count')\n",
    "plt.title('Emotion Counts')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos_neg_df = df[(df['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (df['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_neut_combine_tweets = neg_neut_combine['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_neut_combine_sentiment = neg_neut_combine['pos_flag'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sentiment_count = neg_neut_combine_sentiment.value_counts()\n",
    "print(f'Sentiment count: \\n{full_sentiment_count}')\n",
    "print(f'Positive: {round(full_sentiment_count[0]/sum(full_sentiment_count)*100,1)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(neg_neut_combine_tweets,neg_neut_combine_sentiment, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=neg_neut_combine_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_baseline = X_train[:int(len(X_train) * 0.8)]\n",
    "y_train_baseline = y_train[:int(len(y_train) * 0.8)]\n",
    "X_val_baseline = X_train[-int(len(X_train) * 0.2):]\n",
    "y_val_baseline = y_train[-int(len(y_train) * 0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train\n",
    "clf.fit(X_train_baseline, y_train_baseline)\n",
    "\n",
    "y_pred = clf.predict(X_val_baseline)\n",
    "print(confusion_matrix(y_pred, y_val_baseline))\n",
    "\n",
    "coef = classifier.coef_[0]\n",
    "top_pos_idx = coef.argsort()[-10:][::-1]  # most positive\n",
    "top_neg_idx = coef.argsort()[:10]         # most negative\n",
    "\n",
    "\n",
    "print(\"Top words for positive class:\", feature_names[top_pos_idx])\n",
    "print(\"Top words for negative/neutral class:\", feature_names[top_neg_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "ave_type = 'macro'\n",
    "baseline_accuracy = accuracy_score(y_pred, y_val_baseline)\n",
    "baseline_f1 = f1_score(y_pred, y_val_baseline, average=ave_type)\n",
    "print(baseline_accuracy)\n",
    "print(baseline_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate comparison score based on always guessing majority class (negative/neutral)\n",
    "y_pred2 = [0]*len(y_val_baseline)\n",
    "majority_guess_f1 = f1_score(y_val_baseline, y_pred2, average=ave_type)\n",
    "majority_guess_acc = accuracy_score(y_val_baseline, y_pred2)\n",
    "\n",
    "categories = ['Always Guess Negative/Neutral', 'Logistic Regression Model']\n",
    "f1s = [majority_guess_f1,baseline_f1]\n",
    "plt.bar(categories, f1s)\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylim(0, 0.8)\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 Scores')\n",
    "\n",
    "# Show values on top of bars (optional)\n",
    "for i, count in enumerate(f1s):\n",
    "    plt.text(i, count+.02, str(round(count,3)), ha='center')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipe(X,y, num_words=2000, \n",
    "             remove_stopwords=False, \n",
    "             lemmatize=False, \n",
    "             num_epochs=200, \n",
    "             show_results=False, \n",
    "             balance_classes=False):\n",
    "\n",
    "    # Compute weights to balance positive and negative comments to combat data imbalance\n",
    "    classes = np.unique(y)\n",
    "    class_weights = [1]*len(classes)\n",
    "    if balance_classes:\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=classes,\n",
    "            y=y)\n",
    "#         if len(classes) == 3:\n",
    "#             class_weights=[1/.15,1/.6,1/.4]\n",
    "\n",
    "    class_weights = dict(zip(classes,class_weights))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 1: Custom text prep transformer\n",
    "    # ---------------------------\n",
    "\n",
    "    class TextPrep(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.remove_stopwords = remove_stopwords\n",
    "            self.lemmatize = lemmatize\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        def get_wordnet_pos(self, treebank_tag):\n",
    "            \"\"\"Map POS tag to WordNet format for better lemmatization\"\"\"\n",
    "            if treebank_tag.startswith(\"J\"):\n",
    "                return wordnet.ADJ\n",
    "            elif treebank_tag.startswith(\"V\"):\n",
    "                return wordnet.VERB\n",
    "            elif treebank_tag.startswith(\"N\"):\n",
    "                return wordnet.NOUN\n",
    "            elif treebank_tag.startswith(\"R\"):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return wordnet.NOUN  # fallback\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = X.ravel()\n",
    "            X = [str(x) for x in X]\n",
    "\n",
    "            processed = []\n",
    "            for text in X:\n",
    "                words = word_tokenize(text)\n",
    "\n",
    "                # Remove stopwords if enabled\n",
    "                if self.remove_stopwords:\n",
    "                    words = [w for w in words if w.lower() not in self.stop_words]\n",
    "\n",
    "                # Lemmatization if enabled\n",
    "                if self.lemmatize:\n",
    "                    pos_tags = pos_tag(words)\n",
    "                    words = [self.lemmatizer.lemmatize(w, self.get_wordnet_pos(tag)) \n",
    "                             for w, tag in pos_tags]\n",
    "                processed.append(\" \".join(words))\n",
    "\n",
    "            return processed        \n",
    "        \n",
    "    # ---------------------------\n",
    "    # Step 2: Wrap tokenizer\n",
    "    # ---------------------------\n",
    "    class KerasTokenizer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, mode=\"binary\"):\n",
    "            self.num_words = num_words\n",
    "            self.mode = mode\n",
    "            self.tokenizer = Tokenizer(num_words=self.num_words)\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.tokenizer.fit_on_texts(X)\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            return self.tokenizer.texts_to_matrix(X, mode=self.mode)    \n",
    "      \n",
    "    # ---------------------------\n",
    "    # Step 3: Build model\n",
    "    # ---------------------------\n",
    "    def build_model(input_dim, n_classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, activation=\"relu\", input_shape=(input_dim,)))\n",
    "        model.add(Dense(25, activation=\"relu\"))\n",
    "        model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=['accuracy'],\n",
    "            weighted_metrics=[]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 4: Setup pipeline\n",
    "    # ---------------------------\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"prep\", TextPrep()),\n",
    "        (\"tok\", KerasTokenizer()),\n",
    "        (\"clf\", KerasClassifier(\n",
    "            build_fn=lambda: build_model(\n",
    "                input_dim=pipeline.named_steps[\"tok\"].num_words, \n",
    "                n_classes=n_classes),\n",
    "            epochs=num_epochs,\n",
    "            batch_size=256,\n",
    "            verbose=0,\n",
    "            validation_split=0.2, \n",
    "            callbacks=[EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True)],\n",
    "            class_weight=class_weights)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 5: Fit\n",
    "    # ---------------------------\n",
    "    pipeline.fit(X, y)\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Step 6: Test results\n",
    "    # ---------------------------   \n",
    "    \n",
    "    # Access history directly (no `.history`)\n",
    "    pipe_history = pipeline.named_steps[\"clf\"].history_\n",
    "\n",
    "#     test_acc = pipeline.score(X_test, y_test)\n",
    "#     print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "    X_val = X[-int(len(X) * 0.2):]\n",
    "    y_val = y[-int(len(y) * 0.2):]\n",
    "\n",
    "    # Get predictions and F1 score\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    pipe_f1 = f1_score(y_val, y_pred, average=ave_type)\n",
    "#     print(y_val.value_counts())\n",
    "#     print('\\nPredictions:')\n",
    "#     display(pd.DataFrame(y_pred).value_counts())\n",
    "\n",
    "    # Detailed classification report\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    \n",
    "    if show_results:\n",
    "        # Confusion matrix\n",
    "        print(confusion_matrix(y_val, y_pred))\n",
    "        # Plot accuracy\n",
    "        print(\"Max Val Accuracy:\", max(pipe_history[\"val_accuracy\"]), accuracy_score(y_val, y_pred))\n",
    "        print(\"F1 score: \", pipe_f1)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(pipe_history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "        if \"val_accuracy\" in pipe_history:\n",
    "            plt.plot(pipe_history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "        plt.title(\"Epoch vs Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return max(pipe_history[\"val_accuracy\"]), pipe_f1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_check (X_par, y_par, num_words_list, plot=True, balance_classes=False, show_results=False):\n",
    "    \n",
    "    results=pd.DataFrame(columns=[\"number_of_words\",\"val_accuracy\", 'remove_stopwords', 'lemmatize', 'f1'])\n",
    "    i=1\n",
    "    stop_tf=False\n",
    "    lem_tf=False\n",
    "#     for j in range(3):\n",
    "    for x in num_words_list:\n",
    "        for stop_tf in [False,True]:\n",
    "            for lem_tf in [False,True]:\n",
    "                acc, f1=run_pipe(X_par,y_par, \n",
    "                                 num_words=x,\n",
    "                                 remove_stopwords=stop_tf, \n",
    "                                 lemmatize=lem_tf, \n",
    "                                 balance_classes=balance_classes, \n",
    "                                 show_results=show_results)\n",
    "\n",
    "                print(f'({i} of {len(num_words_list)*4}) Stopwords: {stop_tf}, Lemmatize: {lem_tf}, Number of words: {x}, Val accuracy: {round(acc,4)}, F1 score: {round(f1,4)}')\n",
    "                new_row_data = {'number_of_words': [x], 'val_accuracy': [acc], 'remove_stopwords': [stop_tf], 'lemmatize': [lem_tf], 'f1': [f1]}\n",
    "                new_row = pd.DataFrame(new_row_data)\n",
    "                results = pd.concat([results,new_row], ignore_index=True)\n",
    "                i += 1\n",
    "    results[\"config\"] = results.apply(\n",
    "        lambda row: f\"Stopwords={row['remove_stopwords']}, Lemmatize={row['lemmatize']}\", axis=1)\n",
    "\n",
    "    # Add baseline f1 score (based on always guessing neutral/positive)\n",
    "#     y_val2 = y_par[-int(len(y_par) * 0.2):]\n",
    "#     y_pred2 = [0]*len(y_val2)\n",
    "#     baseline_f1 = f1_score(y_val2, y_pred2, average=ave_type)\n",
    "#     baseline_acc = accuracy_score(y_val2, y_pred2)\n",
    "    val_sentiment_count = y_train[-int(len(y_train) * 0.2):].value_counts()\n",
    "    print(f'Sentiment count: \\n{val_sentiment_count}')\n",
    "    val_sentiment_count[0]/sum(val_sentiment_count)\n",
    "\n",
    "    for num_word in num_words_list:\n",
    "        baseline_row_data = {'number_of_words': [num_word], 'val_accuracy': [baseline_accuracy], 'f1': [baseline_f1], 'config': 'Baseline Linear Regression Model'}\n",
    "        baseline_rows = pd.DataFrame(baseline_row_data)\n",
    "        results = pd.concat([results,baseline_rows], ignore_index=True)\n",
    "    \n",
    "    results = results.sort_values(\"number_of_words\")\n",
    "\n",
    "    if plot:\n",
    "        fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "        sns.lineplot(\n",
    "            data=results,\n",
    "            x=\"number_of_words\",\n",
    "            y=\"val_accuracy\",\n",
    "            hue=\"config\",\n",
    "            style=\"config\",\n",
    "            ax=ax1,\n",
    "            markers=True,\n",
    "#             estimator=None,\n",
    "            dashes=False)\n",
    "        \n",
    "        plt.title(\"Validation Accuracy vs Number of Words\", fontsize=14)\n",
    "        plt.xlabel(\"Number of Words\", fontsize=12)\n",
    "        plt.ylabel(\"Validation Accuracy\", fontsize=12)\n",
    "        plt.legend(title=\"Preprocessing Config\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        fig, ax2 = plt.subplots(figsize=(10,6))\n",
    "        \n",
    "        sns.lineplot(\n",
    "            data=results,\n",
    "            x=\"number_of_words\",\n",
    "            y=\"f1\",\n",
    "            hue=\"config\",\n",
    "            style=\"config\",\n",
    "            ax=ax2,\n",
    "            markers=True,\n",
    "            dashes=True)\n",
    "        plt.title(\"Validation F1 Score vs Number of Words\", fontsize=14)\n",
    "        plt.xlabel(\"Number of Words\", fontsize=12)\n",
    "        plt.ylabel(\"Validation F1 Score\", fontsize=12)\n",
    "        plt.legend(title=\"Preprocessing Config\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment_count = y_test.value_counts()\n",
    "print(f'Sentiment count: \\n{test_sentiment_count}')\n",
    "test_sentiment_count[0]/sum(test_sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentiment_count = y_train[-int(len(y_train) * 0.2):].value_counts()\n",
    "print(f'Sentiment count: \\n{val_sentiment_count}')\n",
    "val_sentiment_count[0]/sum(val_sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train,y_train, num_words_list, show_results=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train,y_train, num_words_list, balance_classes=True, show_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val2 = y_train[-int(len(y_train) * 0.2):]\n",
    "\n",
    "# Get predictions and F1 score\n",
    "y_pred2 = [0]*len(y_val2)\n",
    "f1_score(y_val2, y_pred2, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_pipe(X_train,y_train, num_words=500, num_epochs=300, remove_stopwords=False, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# don't use X_test, y_test in final version. print val results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add neutral/can't tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand_df = df[df['is_there_an_emotion_directed_at_a_brand_or_product'] != \"I can't tell\"].sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweets = rand_df['tweet_text']\n",
    "sentiment = rand_df['is_there_an_emotion_directed_at_a_brand_or_product'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(tweets,sentiment, test_size=0.15, random_state=7, stratify=sentiment)\n",
    "# X_val_all = X_train_all[-int(len(X_train_all) * 0.2):]\n",
    "# y_val_all = y_train_all[-int(len(y_train_all) * 0.2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "round(sentiment.value_counts()/len(sentiment)*100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "round(y_train_all.value_counts()/len(y_train_all)*100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "round(y_test_all.value_counts()/len(y_test_all)*100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_val_all = y_train_all[-int(len(y_train_all) * 0.2):]\n",
    "round(y_val_all.value_counts()/len(y_val_all)*100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train_all,y_train_all, num_words_list, balance_classes=False, show_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "762/(31+71+595+55+762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train_all,y_train_all, num_words_list, balance_classes=True, show_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_val_all.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
