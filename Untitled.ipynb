{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mleroi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mleroi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9065 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9065 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['tweet_text'], inplace=True)\n",
    "df.drop_duplicates(subset=['tweet_text'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_df = df[(df['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (df['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_tweets = pos_neg_df['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_sentiment = pos_neg_df['is_there_an_emotion_directed_at_a_brand_or_product'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment count: \n",
      "Positive emotion    2968\n",
      "Negative emotion     569\n",
      "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64\n",
      "Positive: 83.9%\n"
     ]
    }
   ],
   "source": [
    "sentiment_count = pos_neg_sentiment.value_counts()\n",
    "print(f'Sentiment count: \\n{sentiment_count}')\n",
    "print(f'Positive: {round(sentiment_count[0]/sum(sentiment_count)*100,1)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pos_neg_tweets,pos_neg_sentiment, test_size=0.15, random_state=42, stratify=pos_neg_sentiment)\n",
    "X_val = X_train[-int(len(X_train) * 0.2):]\n",
    "y_val = y_train[-int(len(y_train) * 0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Multiclass F1 Score (macro average) for Keras.\n",
    "    Works with sparse categorical crossentropy labels.\n",
    "    \"\"\"\n",
    "    # Convert y_true to one-hot\n",
    "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])\n",
    "    \n",
    "    # Binarize predictions\n",
    "    y_pred = tf.one_hot(tf.argmax(y_pred, axis=-1), depth=tf.shape(y_pred)[-1])\n",
    "    \n",
    "    # True positives, false positives, false negatives\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, \"float\"), axis=0)\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, \"float\"), axis=0)\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), \"float\"), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "  \n",
    "    # Macro average (mean across classes)\n",
    "    return tf.reduce_mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipe(X,y, num_words=2000, \n",
    "             remove_stopwords=False, \n",
    "             lemmatize=False, \n",
    "             num_epochs=200, \n",
    "             show_results=False, \n",
    "             balance_classes=False):\n",
    "\n",
    "    # Compute weights to balance positive and negative comments to combat data imbalance\n",
    "    classes = np.unique(y)\n",
    "    class_weights = [1]*len(classes)\n",
    "    if balance_classes:\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=classes,\n",
    "            y=y)\n",
    "        \n",
    "    class_weights = dict(zip(classes,class_weights))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 1: Custom text prep transformer\n",
    "    # ---------------------------\n",
    "\n",
    "    class TextPrep(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.remove_stopwords = remove_stopwords\n",
    "            self.lemmatize = lemmatize\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        def get_wordnet_pos(self, treebank_tag):\n",
    "            \"\"\"Map POS tag to WordNet format for better lemmatization\"\"\"\n",
    "            if treebank_tag.startswith(\"J\"):\n",
    "                return wordnet.ADJ\n",
    "            elif treebank_tag.startswith(\"V\"):\n",
    "                return wordnet.VERB\n",
    "            elif treebank_tag.startswith(\"N\"):\n",
    "                return wordnet.NOUN\n",
    "            elif treebank_tag.startswith(\"R\"):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return wordnet.NOUN  # fallback\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = X.ravel()\n",
    "            X = [str(x) for x in X]\n",
    "\n",
    "            processed = []\n",
    "            for text in X:\n",
    "                words = word_tokenize(text)\n",
    "\n",
    "                # Remove stopwords if enabled\n",
    "                if self.remove_stopwords:\n",
    "                    words = [w for w in words if w.lower() not in self.stop_words]\n",
    "\n",
    "                # Lemmatization if enabled\n",
    "                if self.lemmatize:\n",
    "                    pos_tags = pos_tag(words)\n",
    "                    words = [self.lemmatizer.lemmatize(w, self.get_wordnet_pos(tag)) \n",
    "                             for w, tag in pos_tags]\n",
    "                processed.append(\" \".join(words))\n",
    "\n",
    "            return processed        \n",
    "        \n",
    "    # ---------------------------\n",
    "    # Step 2: Wrap tokenizer\n",
    "    # ---------------------------\n",
    "    class KerasTokenizer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, mode=\"binary\"):\n",
    "            self.num_words = num_words\n",
    "            self.mode = mode\n",
    "            self.tokenizer = Tokenizer(num_words=self.num_words)\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.tokenizer.fit_on_texts(X)\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            return self.tokenizer.texts_to_matrix(X, mode=self.mode)    \n",
    "      \n",
    "    # ---------------------------\n",
    "    # Step 3: Build model factory\n",
    "    # ---------------------------\n",
    "    def build_model(input_dim, n_classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, activation=\"relu\", input_shape=(input_dim,)))\n",
    "        model.add(Dense(25, activation=\"relu\"))\n",
    "        model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=['accuracy'],\n",
    "            weighted_metrics=[]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 4: Setup pipeline\n",
    "    # ---------------------------\n",
    "    n_classes = len(np.unique(y_train))  # adjust to your labels\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"prep\", TextPrep()),\n",
    "        (\"tok\", KerasTokenizer()),\n",
    "        (\"clf\", KerasClassifier(\n",
    "            build_fn=lambda: build_model(\n",
    "                input_dim=pipeline.named_steps[\"tok\"].num_words, \n",
    "                n_classes=n_classes\n",
    "            ),\n",
    "            epochs=num_epochs,\n",
    "            batch_size=256,\n",
    "            verbose=0,\n",
    "            validation_split=0.2, \n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)],\n",
    "            class_weight=class_weights\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 5: Fit\n",
    "    # ---------------------------\n",
    "    pipeline.fit(X_train, y_train)\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Step 6: Test results\n",
    "    # ---------------------------   \n",
    "    \n",
    "    # Access history directly (no `.history`)\n",
    "    pipe_history = pipeline.named_steps[\"clf\"].history_\n",
    "\n",
    "#     test_acc = pipeline.score(X_test, y_test)\n",
    "#     print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "    # Get predictions and F1 score\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    pipe_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    \n",
    "    # Detailed classification report\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    if show_results:\n",
    "        # Confusion matrix\n",
    "        print(confusion_matrix(y_val, y_pred))\n",
    "        # Plot accuracy\n",
    "        print(\"Max Val Accuracy:\", max(pipe_history[\"val_accuracy\"]), accuracy_score(y_val, y_pred))\n",
    "        print(\"F1 score: \", pipe_f1)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(pipe_history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "        if \"val_accuracy\" in pipe_history:\n",
    "            plt.plot(pipe_history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "        plt.title(\"Epoch vs Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return max(pipe_history[\"val_accuracy\"]), pipe_f1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_check (X_train, y_train, num_words_list, plot=True, balance_classes=False):\n",
    "    \n",
    "    results=pd.DataFrame(columns=[\"number_of_words\",\"val_accuracy\", 'remove_stopwords', 'lemmatize', 'f1'])\n",
    "    for x in num_words_list:\n",
    "        for stop_tf in [False,True]:\n",
    "            for lem_tf in [False,True]:\n",
    "                acc, f1=run_pipe(X_train,y_train, num_words=x,remove_stopwords=stop_tf, lemmatize=lem_tf, balance_classes=balance_classes)\n",
    "#                 print(f'Stopwords: {stop_tf}, Lemmatize: {lem_tf}, Number of words: {x}, Val accuracy: {round(acc,4)}, F1 score: {round(f1,4)}')\n",
    "                new_row_data = {'number_of_words': [x], 'val_accuracy': [acc], 'remove_stopwords': [stop_tf], 'lemmatize': [lem_tf], 'f1': [f1]}\n",
    "                new_row = pd.DataFrame(new_row_data)\n",
    "                results = pd.concat([results,new_row], ignore_index=True)\n",
    "    results[\"config\"] = results.apply(\n",
    "        lambda row: f\"Stopwords={row['remove_stopwords']}, Lemmatize={row['lemmatize']}\", axis=1\n",
    "    )\n",
    "\n",
    "    results = results.sort_values(\"number_of_words\")\n",
    "\n",
    "    if plot:\n",
    "        fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "        sns.lineplot(\n",
    "            data=results,\n",
    "            x=\"number_of_words\",\n",
    "            y=\"val_accuracy\",\n",
    "            hue=\"config\",\n",
    "            style=\"config\",\n",
    "            ax=ax1,\n",
    "            markers=True,\n",
    "            dashes=False)\n",
    "        \n",
    "        plt.title(\"Validation Accuracy vs Number of Words\", fontsize=14)\n",
    "        plt.xlabel(\"Number of Words\", fontsize=12)\n",
    "        plt.ylabel(\"Validation Accuracy\", fontsize=12)\n",
    "        plt.legend(title=\"Preprocessing Config\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        fig, ax2 = plt.subplots(figsize=(10,6))\n",
    "        \n",
    "        sns.lineplot(\n",
    "            data=results,\n",
    "            x=\"number_of_words\",\n",
    "            y=\"f1\",\n",
    "            hue=\"config\",\n",
    "            style=\"config\",\n",
    "            ax=ax2,\n",
    "            markers=True,\n",
    "            dashes=True)\n",
    "        plt.title(\"Validation F1 Score vs Number of Words\", fontsize=14)\n",
    "        plt.xlabel(\"Number of Words\", fontsize=12)\n",
    "        plt.ylabel(\"Validation F1 Score\", fontsize=12)\n",
    "        plt.legend(title=\"Preprocessing Config\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment count: \n",
      "Positive emotion    446\n",
      "Negative emotion     85\n",
      "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.839924670433145"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_count = y_test.value_counts()\n",
    "print(f'Sentiment count: \\n{sentiment_count}')\n",
    "sentiment_count[0]/sum(sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train,y_train, num_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add another preprocessing step? There was somethng i was thinking about that i can't remember now\n",
    "Add step to artificially increase negative comments. can't remember the name for it. Unbalanced data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train,y_train, num_words_list, balance_classes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 50  48]\n",
      " [ 29 474]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8720930218696594, 0.4567858576774597)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipe(X_train,y_train, num_words=500, num_epochs=300, remove_stopwords=False, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# don't use X_test, y_test in final version. print val results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add neutral/can't tell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
