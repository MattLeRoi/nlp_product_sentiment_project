{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['tweet_text'], inplace=True)\n",
    "df.drop_duplicates(subset=['tweet_text'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_df = df[(df['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (df['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_tweets = pos_neg_df['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_sentiment = pos_neg_df['is_there_an_emotion_directed_at_a_brand_or_product'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count = pos_neg_sentiment.value_counts()\n",
    "print(f'Sentiment count: \\n{sentiment_count}')\n",
    "sentiment_count[0]/sum(sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pos_neg_tweets,pos_neg_sentiment, test_size=0.15, random_state=42, stratify=pos_neg_sentiment)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_pipeline(pipeline, X, y, name=\"Test\"):\n",
    "#     \"\"\"Evaluate an sklearn pipeline with a KerasClassifier at the end.\"\"\"\n",
    "    \n",
    "#     print(f\"--- {name} Evaluation ---\")\n",
    "    \n",
    "#     # sklearn accuracy via pipeline.score\n",
    "#     acc = pipeline.score(X, y)\n",
    "#     print(f\"{name} Accuracy (pipeline.score): {acc:.4f}\")\n",
    "    \n",
    "#     # Predictions\n",
    "#     y_pred = pipeline.predict(X)\n",
    "    \n",
    "#     # sklearn metrics\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(y, y_pred))\n",
    "    \n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion_matrix(y, y_pred))\n",
    "    \n",
    "#     # Keras evaluation (loss + acc)\n",
    "#     model = pipeline.named_steps[\"clf\"].model\n",
    "#     Xt = pipeline.named_steps[\"tok\"].transform(\n",
    "#         pipeline.named_steps[\"prep\"].transform(X)\n",
    "#     )\n",
    "#     loss, keras_acc = model.evaluate(Xt, y, verbose=0)\n",
    "#     print(f\"\\nKeras evaluate -> Loss: {loss:.4f}, Accuracy: {keras_acc:.4f}\")\n",
    "\n",
    "#     return {\n",
    "#         \"sklearn_acc\": acc,\n",
    "#         \"keras_loss\": loss,\n",
    "#         \"keras_acc\": keras_acc,\n",
    "#         \"y_pred\": y_pred\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipe(X,y,num_words=2000, remove_stopwords=False, lemmatize=False, num_epochs=200, show_results=False, balance_classes=False):\n",
    "\n",
    "    # Compute weights to balance positive and negative comments to combat data imbalance\n",
    "    classes = np.unique(y)\n",
    "    class_weights = [1]\n",
    "    if balance_classes:\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=classes,\n",
    "            y=y\n",
    "        )\n",
    "        \n",
    "    class_weights = dict(zip(classes,class_weights))\n",
    "    print(class_weights)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 1: Custom text prep transformer\n",
    "    # ---------------------------\n",
    "\n",
    "    class TextPrep(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.remove_stopwords = remove_stopwords\n",
    "            self.lemmatize = lemmatize\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        def get_wordnet_pos(self, treebank_tag):\n",
    "            \"\"\"Map POS tag to WordNet format for better lemmatization\"\"\"\n",
    "            if treebank_tag.startswith(\"J\"):\n",
    "                return wordnet.ADJ\n",
    "            elif treebank_tag.startswith(\"V\"):\n",
    "                return wordnet.VERB\n",
    "            elif treebank_tag.startswith(\"N\"):\n",
    "                return wordnet.NOUN\n",
    "            elif treebank_tag.startswith(\"R\"):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return wordnet.NOUN  # fallback\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = X.ravel()\n",
    "            X = [str(x) for x in X]\n",
    "\n",
    "            processed = []\n",
    "            for text in X:\n",
    "                words = word_tokenize(text)\n",
    "\n",
    "                # Remove stopwords if enabled\n",
    "                if self.remove_stopwords:\n",
    "                    words = [w for w in words if w.lower() not in self.stop_words]\n",
    "\n",
    "                # Lemmatization if enabled\n",
    "                if self.lemmatize:\n",
    "                    pos_tags = pos_tag(words)\n",
    "                    words = [self.lemmatizer.lemmatize(w, self.get_wordnet_pos(tag)) \n",
    "                             for w, tag in pos_tags]\n",
    "                processed.append(\" \".join(words))\n",
    "\n",
    "            return processed        \n",
    "        \n",
    "    # ---------------------------\n",
    "    # Step 2: Wrap tokenizer\n",
    "    # ---------------------------\n",
    "    class KerasTokenizer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, mode=\"binary\"):\n",
    "            self.num_words = num_words\n",
    "            self.mode = mode\n",
    "            self.tokenizer = Tokenizer(num_words=self.num_words)\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.tokenizer.fit_on_texts(X)\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            return self.tokenizer.texts_to_matrix(X, mode=self.mode)    \n",
    "      \n",
    "    # ---------------------------\n",
    "    # Step 3: Build model factory\n",
    "    # ---------------------------\n",
    "    def build_model(input_dim, n_classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, activation=\"relu\", input_shape=(input_dim,)))\n",
    "        model.add(Dense(25, activation=\"relu\"))\n",
    "        model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=['accuracy'],\n",
    "            weighted_metrics=[]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 4: Setup pipeline\n",
    "    # ---------------------------\n",
    "    n_classes = len(np.unique(y_train))  # adjust to your labels\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"prep\", TextPrep()),\n",
    "        (\"tok\", KerasTokenizer()),\n",
    "        (\"clf\", KerasClassifier(\n",
    "            build_fn=lambda: build_model(\n",
    "                input_dim=pipeline.named_steps[\"tok\"].num_words, \n",
    "                n_classes=n_classes\n",
    "            ),\n",
    "            epochs=num_epochs,\n",
    "            batch_size=256,\n",
    "            verbose=0,\n",
    "            validation_split=0.2, \n",
    "            callbacks=[EarlyStopping(monitor=\"val_accuracy\", patience=15, restore_best_weights=True)],\n",
    "            class_weight=class_weights\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 5: Fit\n",
    "    # ---------------------------\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Step 6: Test results\n",
    "    # ---------------------------   \n",
    "    \n",
    "    # Access history directly (no `.history`)\n",
    "    pipe_history = pipeline.named_steps[\"clf\"].history_\n",
    "\n",
    "#     test_acc = pipeline.score(X_test, y_test)\n",
    "#     print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Detailed classification report\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    if show_results:\n",
    "        # Plot accuracy\n",
    "        print(\"Max Val Accuracy:\", max(pipe_history[\"val_accuracy\"]))\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(pipe_history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "        if \"val_accuracy\" in pipe_history:\n",
    "            plt.plot(pipe_history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "        plt.title(\"Epoch vs Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return max(pipe_history[\"val_accuracy\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_check (X_train, y_train, num_words_list, plot=True, balance_classes=False):\n",
    "    \n",
    "    results=pd.DataFrame(columns=[\"number_of_words\",\"val_accuracy\", 'remove_stopwords', 'lemmatize'])\n",
    "    for x in num_words_list:\n",
    "        for stop_tf in [False,True]:\n",
    "            for lem_tf in [False,True]:\n",
    "                acc=round(run_pipe(X_train,y_train, num_words=x,remove_stopwords=stop_tf, lemmatize=lem_tf, balance_classes=balance_classes),4)\n",
    "                print(f'stopwords: {stop_tf}, lemmatize: {lem_tf}, number of words: {x}, accuracy: {acc}')\n",
    "                new_row_data = {'number_of_words': [x], 'val_accuracy': [acc], 'remove_stopwords': [stop_tf], 'lemmatize': [lem_tf]}\n",
    "                new_row = pd.DataFrame(new_row_data)\n",
    "                results = pd.concat([results,new_row], ignore_index=True)\n",
    "    results[\"config\"] = results.apply(\n",
    "        lambda row: f\"Stopwords={row['remove_stopwords']}, Lemmatize={row['lemmatize']}\", axis=1\n",
    "    )\n",
    "\n",
    "    results = results.sort_values(\"number_of_words\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.lineplot(\n",
    "            data=results,\n",
    "            x=\"number_of_words\",\n",
    "            y=\"val_accuracy\",\n",
    "            hue=\"config\",\n",
    "            style=\"config\",\n",
    "            markers=True,\n",
    "            dashes=False\n",
    "        )\n",
    "\n",
    "        plt.title(\"Validation Accuracy vs Number of Words\", fontsize=14)\n",
    "        plt.xlabel(\"Number of Words\", fontsize=12)\n",
    "        plt.ylabel(\"Validation Accuracy\", fontsize=12)\n",
    "        plt.legend(title=\"Preprocessing Config\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count = y_test.value_counts()\n",
    "print(f'Sentiment count: \\n{sentiment_count}')\n",
    "sentiment_count[0]/sum(sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train,y_train, num_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add another preprocessing step? There was somethng i was thinking about that i can't remember now\n",
    "Add step to artificially increase negative comments. can't remember the name for it. Unbalanced data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_list = [300, 1000, 2000, 5000, 10000]\n",
    "results = parameter_check(X_train,y_train, num_words_list, balance_classes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipe(X_train,y_train, num_words=500, num_epochs=300, remove_stopwords=False, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# don't use X_test, y_test in final version. print val results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add neutral/can't tell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
